{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b90351-d5e5-4d58-a7b7-52dec9b45cfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Master Utility Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149f404-39cb-47af-ab92-e05c6a9452bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Used in Conjunction with other notebooks for analysis and plotting** <br>\n",
    "This notebook contains the definitions for the functions used in the CORDEX-Australasia Benchmarking Framework Analysis which has been accepted as the following manuscript: <br>\n",
    "Isphording, R.N., L.V. Alexander, M. Bador, D. Green, J. P. Evans, and S. Wales. A Standardized Benchmarking Framework to Assess Downscaled Precipitation Simulations. Accepted in Journal of Climate. in revision <br>\n",
    "These functions largely assume a common variable name between model and observation files (such as Climpact output files; see https://climpact-sci.org/). <br>\n",
    "Author: Rachael N. Isphording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3502f169-4890-4dba-afaa-3d10e05d1de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules \n",
    "import pandas as pd\n",
    "import os\n",
    "import fnmatch\n",
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from astropy.stats import circcorrcoef\n",
    "from astropy import units as u\n",
    "from scipy.stats import bootstrap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfebe26-e130-4cb9-be83-b2275c796620",
   "metadata": {
    "tags": []
   },
   "source": [
    "## File Input Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7685453-5431-4db1-9531-6b1553360838",
   "metadata": {
    "tags": []
   },
   "source": [
    "To get data from different directories within my CORDEX-Australasia database, f-strings are used in the analysis notebooks. <br>\n",
    "**This will likely need to be updated based on your database structure** <br>\n",
    "- The **CORDEX-Australasia** database is set-up so that the Climpact indices follow the path: <br>\n",
    "    - parent_directory/CORDEX-Australasia/data/{rcm-or-gcm}/{grid_type}/{climpact-or-daily-data}/{historical-or-future}/{indice-keyword}/{yr-or-mon}/dataset_file.nc <br><br>\n",
    "- The **observational** dataset (AGCD) is organized differently in the database: <br>\n",
    "    - parent_directory/CORDEX-Australasia/data/{obs}/{grid_type}/{climpact-or-daily-data}/{indice-keyword}/dataset_file_MON-or-ANN.nc\n",
    "\n",
    "**Functions Include:**\n",
    "- Get data model names from dictionaries below\n",
    "- Get subset of models from dictionary defined in analysis notebooks\n",
    "- Get model data paths\n",
    "- Get data from paths\n",
    "    - This includes exceptions to handle the 4-dimensionality of the SPI variable and the \"day\" unit of the CD(W)D and R_mm variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363de8cc-1d91-4bc6-8912-013aeee9e019",
   "metadata": {},
   "source": [
    "### Define names of models\n",
    "I select models from my database based on the GCM and RCM names included in the file name. This will likely to be updated for use in other databases or with other ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ae749d-297c-4595-b6aa-fe3033f49134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List names of forcing GCMs\n",
    "gcm_names = [\n",
    "    \"ACCESS1-0\"\n",
    "    , \"CanESM2\"\n",
    "    , \"CNRM-CM5\"\n",
    "    , \"GFDL-ESM2M\"\n",
    "    , \"HadGEM2-CC\"\n",
    "    , \"HadGEM2-ES\"\n",
    "    , \"MIROC5\"\n",
    "    , \"MPI-ESM-LR\"\n",
    "    , \"MPI-ESM-MR\"\n",
    "    , \"NorESM1-M\"\n",
    "]\n",
    "\n",
    "# List names of RCMs\n",
    "rcm_names = [\n",
    "    \"CCAM-1704\"\n",
    "    , \"CCAM-2008\"\n",
    "    , \"CCLM5-0-15\"\n",
    "    , \"RegCM4-7\"\n",
    "    , \"REMO2015\"\n",
    "    , \"WRF360J\"\n",
    "    , \"WRF360K\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f920104-de8c-440d-9e76-c63bb0e43910",
   "metadata": {},
   "source": [
    "### Get Model Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c1ede1-1dcf-48cf-ac49-33da0d0def2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dictionaries above to get the model names from each file\n",
    "def get_data_name(names, file):\n",
    "    \n",
    "    # Check the model name is in one of the above model lists\n",
    "    for name in names:\n",
    "        if fnmatch.fnmatch(file, \"*\" + name + \"_*\"):\n",
    "            return name\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28383372-f50d-4de8-b831-99dcc86d4ade",
   "metadata": {},
   "source": [
    "### Get Model Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d224a5-2905-4836-82ca-1d98ddfc5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model file paths and add to a Pandas Dataframe \n",
    "def get_model_files(model_data_master_path):\n",
    "    \n",
    "    # Initialise pandas df of model_paths\n",
    "    model_paths = pd.DataFrame(columns=['rcm', 'driving_gcm', 'dataset_path'])\n",
    "\n",
    "    # Loop through the data directory passed to the function, check that the file is a model in the above lists, and get the dataset paths\n",
    "    for model_file in os.listdir(model_data_master_path):\n",
    "    \n",
    "        # Get the RCM and GCM files names\n",
    "        rcm_name = get_data_name(rcm_names, model_file) \n",
    "        driving_gcm_name = get_data_name(gcm_names, model_file)\n",
    "    \n",
    "        # If the RCM or GCM name is invalid (not in list), continue\n",
    "        if rcm_name is None or driving_gcm_name is None: continue\n",
    "    \n",
    "        # Get full path to the dataset \n",
    "        model_complete_file_path = model_data_master_path + model_file\n",
    "            \n",
    "        # Add information to DataFrame\n",
    "        model_paths.loc[len(model_paths.index)] = [rcm_name, driving_gcm_name, model_complete_file_path]\n",
    "            \n",
    "    return model_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ca6f6-61f2-4fa1-82e8-6eca48b0cd12",
   "metadata": {},
   "source": [
    "### Get Model Data Paths for Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41bf5a8b-7bc4-40f4-b05b-bace12272bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model file paths for subset of models from list defined above\n",
    "def get_model_files_subset(model_paths, subset_names):\n",
    "    \n",
    "    # Initialize new Pandas Dataframe to hold data paths for subsets of models\n",
    "    model_paths_subset = pd.DataFrame(columns=['model_name', 'dataset_path'])\n",
    "\n",
    "    for i, row in model_paths.iterrows():\n",
    "        model_name = f'{row[1]}   {row[0]}'\n",
    "    \n",
    "        if model_name in subset_names:\n",
    "            model_paths_subset.loc[len(model_paths_subset.index)] = [model_name, f'{row[2]}']\n",
    "        \n",
    "    return model_paths_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333256c0-ad00-4354-9ba7-04da53e711b9",
   "metadata": {},
   "source": [
    "### Get Data from File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53d5654-dd01-431c-8b8a-70b10d62ad65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract data from files based on user specifications \n",
    "def get_data_from_file(file_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None):\n",
    "    \n",
    "    # If iscale is not given, check other variable exceptions before extracting data\n",
    "    if iscale is None:\n",
    "    \n",
    "        # If the variable is CDD/CWD (Consecutive Dry/Wet Days) or R*mm (number of heavy rain days), the data must be read in differently to convert the variable dtype to float; this list will need to be expanded if temperature indices are incorporated\n",
    "        if variable == 'cdd':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.cdd.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.cdd.attrs['units'] = 'days'\n",
    "            data_var = data_ds.cdd.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "            \n",
    "        elif variable == 'cwd':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.cwd.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.cwd.attrs['units'] = 'days'\n",
    "            data_var = data_ds.cwd.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "    \n",
    "        elif variable == 'r10mm':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.r10mm.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.r10mm.attrs['units'] = 'days'\n",
    "            data_var = data_ds.r10mm.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "        elif variable == 'r20mm':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.r20mm.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.r20mm.attrs['units'] = 'days'\n",
    "            data_var = data_ds.r20mm.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "        elif variable == 'r30mm':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.r30mm.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.r30mm.attrs['units'] = 'days'\n",
    "            data_var = data_ds.r30mm.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "            \n",
    "        elif variable == 'fracprday':\n",
    "            \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.fracprday.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.fracprday.attrs['units'] = 'days'\n",
    "            data_var = data_ds.fracprday.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "        else:\n",
    "    \n",
    "            # Open file and store data as a DataArray\n",
    "            data_ds = xr.open_dataset(file_path)\n",
    "\n",
    "            # Subset data based on user-specified spatiotemporal boundaries\n",
    "            data_subset = data_ds.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "    \n",
    "            # Extract user-specified variable\n",
    "            data_var = getattr(data_subset, variable)\n",
    "   \n",
    "    # Extract appropriate SPI data based on the user-defined scale if the iscale is not None\n",
    "    # (1 for 3-month averaging period, 2 for 6-month averaging period, 3 for 12-month averaging period)\n",
    "    else:\n",
    "        \n",
    "        # Extract user-specified SPI data\n",
    "        data_ds = xr.open_dataset(file_path)\n",
    "        data_var = data_ds.spi.sel(scale=iscale, time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "    if season is None: \n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        data_var = data_var.sel(time=data_var.time.dt.month.isin(season))\n",
    "    \n",
    "    return data_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28793a30-eb6b-4c91-b766-da944b2742c6",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c8ff0-5eb9-4f45-ae67-f45bc5de2396",
   "metadata": {},
   "source": [
    "Functions used when creating plots/figures. <br>\n",
    "**Functions Include:** <br>\n",
    "- Truncating a predefined colormap\n",
    "- Define vertical bands to shade along a time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac1853-8240-4238-8f58-ec69cc201908",
   "metadata": {},
   "source": [
    "### Truncate a color map (skip colors in pre-defined colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "178092ba-0b9f-46b9-b386-7c2ef6bcfec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to be able to skip colors/subset colors within a predefined colormap\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "# Code sourced from https://stackoverflow.com/questions/18926031/how-to-extract-a-subset-of-a-colormap-as-a-new-colormap-in-matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8356b-5437-4f93-a8b1-ff56edc0e521",
   "metadata": {},
   "source": [
    "### Define vertical bands (regions) to shade along a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606d96fb-eee2-4398-936c-85b2456e85d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function to find areas that need to be shaded using a boolean mask as input\n",
    "def fill_vertical_columns(boolean_fill_mask):\n",
    "    \n",
    "    # Find areas in the mask when values change (i.e. boolean switched from True to False and vice versa)\n",
    "    boolean_switch = np.diff(boolean_fill_mask)\n",
    "    \n",
    "    # Find start and end of sections where the boolean fill mask is True (places I want to shade)\n",
    "    region_to_shade, = boolean_switch.nonzero()\n",
    "    \n",
    "    # Handle edge cases where condition starts or ends with True\n",
    "    if boolean_fill_mask[0]:\n",
    "        region_to_shade = np.r_[0, region_to_shade]\n",
    "   \n",
    "    if boolean_fill_mask[-1]:\n",
    "        region_to_shade = np.r_[region_to_shade, len(boolean_fill_mask)]\n",
    "    \n",
    "    # Reshape the result into pairs of start/end indices\n",
    "    region_to_shade = region_to_shade.reshape((-1, 2))\n",
    "    \n",
    "    return region_to_shade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf016c-5231-4f30-8e9f-fa2548cd8ad1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spatially Averaged Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b0b78-bb7a-47da-8f80-37dc754fef56",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following functions calculate spatially averaged metrics, primarily for plotting time series and scatterplots. Areal weighting is incorporated into the functions. <br>\n",
    "'season=None' sets an optional argument for seasonal subsetting based on numeric month values <br>\n",
    "'mask=None' sets an optional argument for masking, where the default is None <br>\n",
    "'scale=None' sets an optional argument for selecting the scale of the 4-D SPI indice. This should only be assigned a value <br>\n",
    "'region_mask=None' sets an optional additional mask for subregions, such as the IPCC regions or NRM regions (assuming the region mask is also a netCDF file. The default is None. <br>\n",
    "'region=None' should be used with the region_mask to specify which region in the region mask. This was created using the IPCC subregions where each region has a numerical value. The default is None. <br>\n",
    "    If the variable is SPI. The scale can be 3, 6, or 12 months. <br>\n",
    "**Functions Include:** <br>\n",
    "- Weighted Spatial Average from Pre-processed Data (such as a climatology)\n",
    "- Spatial Pattern Correlation (Homogenous Variable Names, like Climpact output)\n",
    "- Mean Absolute Percentage Error (Homogenous Variable Names, like Climpact output)\n",
    "- Monthly Averages over time/space to gauge seasonality (Homogenous Variable Names, like Climpact output)\n",
    "- Annual Averages\n",
    "- Normalized Root Mean Square Error for pre-processed data (i.e. maps of climatologies or other calculated values)\n",
    "- Paired Bootstrapping function to calculate the Circular Correlation Coefficient on a random subset of two sets of data\n",
    "- Get Weighted Spatial Average at Default Time Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda4ce4-9695-4f51-999d-d4acfca22a76",
   "metadata": {},
   "source": [
    "### Get Weighted Spatial Average from Preprocessed Data (such as a climatology or model bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3741aaf-ce90-44e1-a32b-39e9a8f32e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_spatial_average_from_data(data, region_mask=None, region=None):\n",
    "    \n",
    "    # Check if there is a mask for a sub-region\n",
    "    if region is None:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data.where(region_mask==region)\n",
    "    \n",
    "    # Compute latitudinally weighted spatially averaged data\n",
    "    # Create latitudinal weights\n",
    "    weights = np.cos(np.deg2rad(data.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    # Weight: apply weighted lats to data\n",
    "    data_weighted = data.weighted(weights)\n",
    "\n",
    "    # Get weighted spatial average\n",
    "    data_weighted_spatial_average = data_weighted.mean(('lon', 'lat'))\n",
    "    \n",
    "    return data_weighted_spatial_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e843407-66a2-46aa-b749-16edc3ac7df1",
   "metadata": {},
   "source": [
    "### Get Spatial Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9e58a9-a5cb-4dcb-9dea-1f4113a113c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spatial pattern correlation (using the Pearson Correlation Coefficient) between model simulation and observational dataset\n",
    "def get_spatial_correlation(model_path, obs_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=None, centered=True, region_mask=None, region=None):\n",
    "\n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        # Get observational dataset\n",
    "        obs_data = get_data_from_file(obs_path, variable, time_slice, lat_slice, lon_slice)\n",
    "\n",
    "        # Get model dataset\n",
    "        model_data = get_data_from_file(model_path, variable, time_slice, lat_slice, lon_slice)\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # Get observational dataset\n",
    "        obs_data = get_data_from_file(obs_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "    \n",
    "        # Get model dataset\n",
    "        model_data = get_data_from_file(model_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "        \n",
    "    # Check if there is a mask for a sub-region\n",
    "    if region is None:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        model_data = model_data.where(region_mask==region)\n",
    "        obs_data = obs_data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        model_data = model_data.where(mask==1)\n",
    "        obs_data = obs_data.where(mask==1)\n",
    "    \n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        if time_slice is None:\n",
    "            obs_climatology = obs_data\n",
    "            model_climatology = model_data\n",
    "        else:\n",
    "            obs_climatology = obs_data.mean(dim='time')\n",
    "            model_climatology = model_data.mean(dim='time')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        obs_climatology = obs_data.sel(time=obs_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        model_climatology = model_data.sel(time=model_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "    \n",
    "    # Calculate weighted pattern correlation and reduce to two decimal points\n",
    "    # Create latitudinal weights\n",
    "    weights = np.cos(np.deg2rad(obs_data.lat))\n",
    "    weights.name = \"weights\"\n",
    "    \n",
    "    obs_weighted_mean = obs_climatology.weighted(weights).mean()\n",
    "    model_weighted_mean = model_climatology.weighted(weights).mean()\n",
    "    weights2d_0 = np.expand_dims(weights.to_numpy(), axis=1)\n",
    "    weights2d = np.repeat(weights2d_0, len(obs_data.lon), axis=1)\n",
    "    \n",
    "    # Check if user wants centered or uncentered correlation; default is centered for rainfall climatologies\n",
    "    # Calculate anomalies for centered correlation\n",
    "    if centered is True:\n",
    "        obs_input = obs_climatology - obs_weighted_mean\n",
    "        model_input = model_climatology - model_weighted_mean\n",
    "    \n",
    "    # Use means for uncentered correlation\n",
    "    elif centered is False:\n",
    "        obs_input = obs_climatology\n",
    "        model_input = model_climatology\n",
    "        \n",
    "    # Calculate weighted covariance and variance\n",
    "    cov = (obs_input*model_input*weights2d).sum(skipna=True)\n",
    "    obs_var = ((obs_input**2)*weights2d).sum(skipna=True)\n",
    "    model_var = ((model_input**2)*weights2d).sum(skipna=True)\n",
    "    \n",
    "    # Calculate spatial correlation\n",
    "    spatial_cor_i = cov / (np.sqrt(obs_var)*np.sqrt(model_var))\n",
    "    \n",
    "    # Round spatial correlation to 2 decimal places\n",
    "    spatial_cor = spatial_cor_i.astype(float).round(2)\n",
    "    \n",
    "    return spatial_cor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b8505-66be-452c-a15c-d5da702a7d6e",
   "metadata": {},
   "source": [
    "### Get Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f4da21-2b81-4b3c-856a-94537d49ed54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Mean absolute Percentage Error (MAPE) between model simulation and observational dataset. This function assumes area weighting and homogenous variable name between datsets\n",
    "def get_mape(model_path, obs_path, variable, time_slice, lat_slice, lon_slice, dataype, season=None, iscale=None, mask=None, region_mask=None, region=None):\n",
    "\n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        # Get observational dataset\n",
    "        obs_data = get_data_from_file(obs_path, variable, time_slice, lat_slice, lon_slice)\n",
    "\n",
    "        # Get model dataset\n",
    "        model_data = get_data_from_file(model_path, variable, time_slice, lat_slice, lon_slice)\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # Get observational dataset\n",
    "        obs_data = get_data_from_file(obs_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "    \n",
    "        # Get model dataset\n",
    "        model_data = get_data_from_file(model_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "        \n",
    "        \n",
    "    # Check data type to properly convert RCM precip data to mm/day from kg/m^2/s\n",
    "    if data_type == \"model\":\n",
    "        model_data = model_data * 86400\n",
    "    \n",
    "    else: \n",
    "         pass  \n",
    "    \n",
    "    # Check if there is a mask for a sub-region\n",
    "    if region is None:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        model_data = model_data.where(region_mask==region)\n",
    "        obs_data = obs_data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        model_data = model_data.where(mask==1)\n",
    "        obs_data = obs_data.where(mask==1)\n",
    "    \n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        obs_climatology = obs_data.mean(dim='time')\n",
    "        model_climatology = model_data.mean(dim='time')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        obs_climatology = obs_data.sel(time=obs_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        model_climatology = model_data.sel(time=model_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "    \n",
    "    # Calculate weights\n",
    "    weights = np.cos(np.deg2rad(obs_data.lat))\n",
    "    weights.name = \"weights\"\n",
    "    weights2d_0 = np.expand_dims(weights.to_numpy(), axis=1)\n",
    "    weights2d = np.repeat(weights2d_0, len(obs_data.lon), axis=1)\n",
    "    \n",
    "    # Convert weights to Xarray for input into correlation function\n",
    "    weights2d_xr = xr.DataArray(weights2d, coords={'lat': obs_data.lat, 'lon': obs_data.lon}, dims=['lat', 'lon'])\n",
    "        \n",
    "    # Calculate Weighted Spatial (i.e. Pattern) Correlation and reduce to 2 decimals\n",
    "    mape_i = xs.mape(obs_climatology, model_climatology, dim=['lat', 'lon'], weights=weights2d_xr, skipna=True)\n",
    "    mape = mape_i.astype(float).round(2)\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa1304-3353-43d3-beee-7b6bc54bb6c0",
   "metadata": {},
   "source": [
    "### Get Monthly Weighted Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07afed97-80c2-46af-b09f-ef19f8f61462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get monthly averages to use for a line plot over a user-specified time period and spatial domain\n",
    "def get_monthly_averages(data_path, variable, time_slice, lat_slice, lon_slice, data_type, iscale=None, mask=None, region_mask=None, region=None):\n",
    "    \n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice)\n",
    "    \n",
    "    else:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "    \n",
    "    \n",
    "    # Check if there is a mask for a sub-region\n",
    "    if region is None:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data.where(region_mask==region) \n",
    "    \n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1)\n",
    "       \n",
    "    \n",
    "    # Check data type to properly convert RCM precip data to mm/day from kg/m^2/s\n",
    "    if data_type == \"model\":\n",
    "        data = data * 86400\n",
    "    \n",
    "    else: \n",
    "         pass\n",
    "    \n",
    "    # Compute latitudinally weighted spatially averaged data\n",
    "    # Create latitudinal weights\n",
    "    weights = np.cos(np.deg2rad(data.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    # Weight: apply weighted lats to data\n",
    "    data_weighted = data.weighted(weights)\n",
    "\n",
    "    # Get weighted averages at each time step\n",
    "    data_weighted_mean = data_weighted.mean(('lon', 'lat'))\n",
    "\n",
    "    # Get weighted monthly averages\n",
    "    data_weighted_mean_mon = data_weighted_mean.groupby('time.month').mean(dim='time')\n",
    "    \n",
    "    return data_weighted_mean_mon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c60b92-a93b-4ed7-a425-0a345e003f0f",
   "metadata": {},
   "source": [
    "### Get Annual Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f053f129-e1ec-41df-82ae-cb91ebba2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get annual averages to use for a line plot over a user-specified time period and spatial domain\n",
    "def get_annual_average(data_path, variable, time_slice, lat_slice, lon_slice, data_type, iscale=None, mask=None, region_mask=None, region=None):\n",
    "    \n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice)\n",
    "    \n",
    "    else:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "    \n",
    "    # Check data type to properly convert daily precip data to mm/day from kg/m^2/s (model data)\n",
    "    if data_type == \"model\":\n",
    "        data = data * 86400\n",
    "    \n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    # Check if we are doing an IPCC sub-region\n",
    "    if region is None:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1)\n",
    "    \n",
    "    # Compute latitudinally weighted spatially averaged data\n",
    "    # Create latitudinal weights\n",
    "    weights = np.cos(np.deg2rad(data.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    # Weight: apply weighted lats to data\n",
    "    data_weighted = data.weighted(weights)\n",
    "\n",
    "    # Get weighted averages at each time step\n",
    "    data_weighted_mean = data_weighted.mean(('lon', 'lat'))\n",
    "\n",
    "    # Get weighted monthly averages\n",
    "    data_weighted_mean_ann = data_weighted_mean.groupby('time.year').mean(dim='time')\n",
    "    \n",
    "    return data_weighted_mean_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6d395-ccb9-4155-a96a-22a97a58db90",
   "metadata": {},
   "source": [
    "### Get the Normalized Root Mean Square Error for pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a93859-0af0-4cba-bb11-dbc1ce6c12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrmse_mean_ppdata(obs_data, model_data):\n",
    "\n",
    "    weights = np.cos(np.deg2rad(obs_data.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    mse = (np.square(np.subtract(model_data, obs_data)).weighted(weights)).mean()\n",
    "    rmse = math.sqrt(mse)\n",
    "    \n",
    "    # Calculate normalized root mean square error using the observational average and reduce to two decimal points\n",
    "    #Normalized using Obs Mean\n",
    "    nrmse_i = rmse/((obs_data.weighted(weights)).mean())\n",
    "    nrmse = np.round(nrmse_i, 2)\n",
    "    \n",
    "    return nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77090591-9d69-4b66-9d68-10bd3931fd5c",
   "metadata": {},
   "source": [
    "### Circular Correlation Coefficient for Bootstrap Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05eba37-79a7-4f22-a988-ba55d464c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_bootstrap(data1, data2, resample_percentage):\n",
    "\n",
    "    # Resample using XX percentage of the data\n",
    "    resample_percentage = resample_percentage\n",
    "    \n",
    "    n = len(data1)\n",
    "    resample_size = int(n * resample_percentage)\n",
    "    \n",
    "    # Get random indices of the subset percentage of the data\n",
    "    indices = np.random.choice(np.arange(n), size=resample_size, replace=False)\n",
    "    \n",
    "    # Resample the original data using the subset of random indices\n",
    "    resampled_data1 = data1[indices]\n",
    "    resampled_data2 = data2[indices]\n",
    "    \n",
    "    # Calculate the circular correlation coefficient \n",
    "    circular_corr = circcorrcoef(resampled_data1, resampled_data2)\n",
    "    \n",
    "    return circular_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5028c03-a19e-4ee3-b974-91b49d503d1f",
   "metadata": {},
   "source": [
    "### Get Weighted Spatial Average at Default Time Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bed0fa1-b543-4219-86b4-e81a62e02fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_spatial_average_at_default_time_step(data_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=None, region_mask=None, region=None):\n",
    "    \n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, season)\n",
    "    \n",
    "    else:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, season, iscale)\n",
    "              \n",
    "    # Check if we are masking with an IPCC sub-region or similar type of mask\n",
    "    if region is None:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask is provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1)\n",
    "    \n",
    "    # Compute latitudinally weighted spatially averaged data\n",
    "    # Create latitudinal weights\n",
    "    weights = np.cos(np.deg2rad(data.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    # Weight: apply weighted lats to data\n",
    "    data_weighted = data.weighted(weights)\n",
    "\n",
    "    # Get weighted averages at each time step\n",
    "    data_weighted_mean = data_weighted.mean(('lon', 'lat'))\n",
    "    \n",
    "    # Return weighted spatial average at default time step\n",
    "    return data_weighted_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0177aee-3751-479b-af72-a5b52f839253",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metrics for Maps (Temporally Averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a49cc7-b6ff-4484-87ae-f65c2e8b5f31",
   "metadata": {},
   "source": [
    "The following functions calculate temporally averaged metrics, primarily for plotting maps. <br>\n",
    "'data_type=None' assumes rainfall units of mm/day. If data_type = 'model' is used, the function converts rainfall units of kg/m^2/s to mm/day. <br>\n",
    "'season=None' sets an optional argument for seasonal subsetting based on numeric month values <br>\n",
    "'scale=None' sets an optional argument for selecting the scale of the 4-D SPI index. This should only be assigned a value if the variable is SPI. The scale can be 3, 6, or 12 months. <br>\n",
    "'mask=None' sets an optional argument for masking, where the default is None <br>\n",
    "'region_mask=None' sets an optional additional mask for subregions, such as the IPCC regions or NRM regions (assuming the region mask is also a netCDF file. The default is None. <br>\n",
    "'region=None' should be used with the region_mask to specify which region in the region mask. This was created using the IPCC subregions where each region has a numerical value. The default is None. <br>\n",
    "**Functions Include:** <br>\n",
    "- Get Bias\n",
    "- Get Climatology\n",
    "- Get the Amplitude of the Annual Cycle at Each Grid Point\n",
    "- Get the Phase of the Annual Cycle at Each Grid Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d69f55-6eeb-41e0-bccc-a8acdab3880d",
   "metadata": {},
   "source": [
    "### Get Model Bias (Model - Obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8de615b-e3dd-45c2-af21-7c4efe5bf485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias(model_path, obs_path, variable, time_slice, lat_slice, lon_slice, data_type=None, season=None, iscale=None, mask=None, region_mask=None, region=None):\n",
    "\n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        # Get observational dataset\n",
    "        obs_data = get_data_from_file(obs_path, variable, time_slice, lat_slice, lon_slice)\n",
    "\n",
    "        # Get model dataset\n",
    "        model_data = get_data_from_file(model_path, variable, time_slice, lat_slice, lon_slice)\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # Get observational dataset\n",
    "        obs_data = get_data_from_file(obs_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "    \n",
    "        # Get model dataset\n",
    "        model_data = get_data_from_file(model_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "        \n",
    "    # Check if we are doing a sub-region\n",
    "    if region is None:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        model_data = model_data.where(region_mask==region)\n",
    "        obs_data = obs_data.where(region_mask==region)\n",
    "        \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        model_data = model_data.where(mask==1) # This is for the combined quality mask; for only land-mask use (mask>50, drop=True)\n",
    "        obs_data = obs_data.where(mask==1)\n",
    "    \n",
    "    # Check if data is model data and convert units to mm/day from kg/m^2/s\n",
    "    if data_type == \"model\":\n",
    "        model_data = model_data * 86400\n",
    "    \n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    # Extract season if one is given\n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        obs_climatology = obs_data.mean(dim='time')\n",
    "        model_climatology = model_data.mean(dim='time')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        obs_climatology = obs_data.sel(time=obs_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        model_climatology = model_data.sel(time=model_data.time.dt.month.isin(season)).mean(dim='time')\n",
    "\n",
    "    # Calculate Model bias\n",
    "    bias = (model_climatology - obs_climatology)\n",
    "    \n",
    "    return bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a5b5c-9634-49ac-aefc-5a30c760b930",
   "metadata": {},
   "source": [
    "### Get Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f6a4aa-9fb1-4dd7-bce3-c3d3baa30291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_climatology(data_path, variable, time_slice, lat_slice, lon_slice, data_type=None, season=None, iscale=None, mask=None, region_mask=None, region=None):\n",
    "\n",
    "    # Get data\n",
    "    data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "    \n",
    "    # Check if we are doing an IPCC or other sub-region\n",
    "    if region is None:\n",
    "      \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data.where(region_mask==region)\n",
    "    \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1) #(mask>50,drop=True) - for only landmask in obs only plot \n",
    "    \n",
    "    # Check if data is model data and convert units to mm/day from kg/m^2/s\n",
    "    if data_type == \"model\":\n",
    "        data = data * 86400\n",
    "    \n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    # Extract season if one is given\n",
    "    if season is None:\n",
    "        \n",
    "        # Calculate Annual climatology datasets\n",
    "        data_climatology = data.mean(dim='time')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Calculate seasonal climatology datasets; the season is defined by the user as a list of month numbers\n",
    "        data_climatology = data.sel(time=data.time.dt.month.isin(season)).mean(dim='time')\n",
    "        \n",
    "    return data_climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efb85f-a5cb-4ceb-88a3-ba1d52637353",
   "metadata": {},
   "source": [
    "### Get Amplitude of Annual Cycle at Each Grid Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cc8cb6-0bbb-4832-8e9d-fef4783ca3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amplitude_of_annual_cycle(file_path, variable, time_slice, lat_slice, lon_slice, data_type=None, iscale=None, mask=None):\n",
    "    \n",
    "    # Get data\n",
    "    data = get_data_from_file(file_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "        \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1)\n",
    "    \n",
    "    # Check data type and convert model data to mm/day from kg/m^2/s\n",
    "    if data_type == \"model\":\n",
    "        data = data * 86400\n",
    "    \n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    # Calculate the Annual Cycle at each grid point\n",
    "    data_ann_cycle = data.groupby('time.month').mean(dim='time')\n",
    "    \n",
    "    # Calculate the Amplitude at each grid point (Max - Mean)\n",
    "    data_amplitude = data_ann_cycle.max(('month')) - data_ann_cycle.mean(('month'))\n",
    "    \n",
    "    return data_amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43508af-2226-4376-a27a-ad644b7df01c",
   "metadata": {},
   "source": [
    "### Get Phase of Annual Cycle at Each Grid Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e1a2e5-9024-45eb-a8b4-e8b423042f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_of_annual_cycle(data_path, variable, time_slice, lat_slice, lon_slice, iscale=None, mask=None):\n",
    "    \n",
    "    # Get data\n",
    "    data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, iscale)\n",
    "        \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1)\n",
    "    \n",
    "    # Calculate the Annual Cycle at each grid point\n",
    "    data_ann_cycle = data.groupby('time.month').mean(dim='time')   \n",
    "    \n",
    "    # Get phase at each grid point (month of maximum)\n",
    "    # Set up empty xarray with correct lat and lon coordinates\n",
    "    data_phase = data.isel(time=0)\n",
    "\n",
    "    # Loop through each grid point and get the phase\n",
    "    for i, j in itertools.product(range(len(data.lat)), range(len(data.lon))):\n",
    "      \n",
    "        # Calculate the month with the maximum value (i.e. phase)\n",
    "        max_mon = np.where(data_ann_cycle.isel(lat=i, lon=j) == data_ann_cycle.isel(lat=i, lon=j).max())\n",
    "        \n",
    "        # Add nan values for grid boxes without data\n",
    "        try:\n",
    "            data_phase[i,j] = max_mon[0].item(0)\n",
    "        except:\n",
    "            data_phase[i,j] = np.nan\n",
    "            \n",
    "    return data_phase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3]",
   "language": "python",
   "name": "conda-env-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
